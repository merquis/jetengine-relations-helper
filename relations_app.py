#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
JetEngine Relations Helper â€“ Streamlit
======================================

â€¢ SecciÃ³n â€œRelaciones CPTâ€ â€” mantiene tus herramientas de reseÃ±as/Alojamientos.  
â€¢ SecciÃ³n â€œScrapingâ€       â€” de momento solo imprime â€œHOLAâ€.
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import base64
import re
from typing import List

import pandas as pd
import requests
import streamlit as st
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIGURACIÃ“N GLOBAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API_BASE = "https://triptoislands.com/wp-json/jet-rel/12"
SEP = re.compile(r"[\s,\.]+")
HEADERS = {"Content-Type": "application/json"}
GOOGLE_SEARCH_URL = "https://www.google.es/search"
USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/123.0.0.0 Safari/537.36"
)

# (Opcional) AutenticaciÃ³n si usas la API REST protegida de tu WP
USER = st.secrets.get("wp_user", "")
APP = st.secrets.get("wp_app_pass", "")
if USER and APP:
    HEADERS["Authorization"] = (
        "Basic " + base64.b64encode(f"{USER}:{APP}".encode()).decode()
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UTILIDADES (scraping rÃ¡pido) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def buscar_en_google(palabra_clave: str) -> List[str]:
    """Devuelve las 5 primeras URLs externas de Google (modo libre)."""
    headers = {"User-Agent": USER_AGENT}
    params = {"q": palabra_clave, "hl": "es", "gl": "es", "num": 10}

    resp = requests.get(GOOGLE_SEARCH_URL, headers=headers, params=params)
    soup = BeautifulSoup(resp.text, "html.parser")

    urls = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        # /url?q=https://sitio.com&sa=...
        if href.startswith("/url?q="):
            clean = href.split("/url?q=")[1].split("&")[0]
            if "google.com" not in clean and "webcache.googleusercontent.com" not in clean:
                urls.append(clean)
        # Enlaces directos (raro, pero pueden aparecer)
        elif href.startswith("http") and "google.com" not in href:
            urls.append(href)

    return urls[:5]


def guardar_urls_en_csv(urls: List[str], filename: str = "urls_resultados.csv"):
    pd.DataFrame(urls, columns=["URL"]).to_csv(filename, index=False)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STREAMLIT UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Relaciones CPT & Scraping", layout="wide")

# â–¸â–¸ Barra lateral â€“ menÃº principal
seccion = st.sidebar.selectbox("Selecciona mÃ³dulo", ("Relaciones CPT", "Scraping"))

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. RELACIONES CPT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
if seccion == "Relaciones CPT":
    st.title("ğŸ› ï¸ Relaciones CPT")

    accion = st.sidebar.radio(
        "Selecciona acciÃ³n",
        (
            "Ver reseÃ±as de alojamiento",
            "AÃ±adir reseÃ±as a alojamiento",
            "Vincular reseÃ±a â†’ alojamiento",
        ),
    )

    # AquÃ­ sigue tu lÃ³gica existenteâ€¦
    # Ejemplo de placeholder:
    if accion == "Ver reseÃ±as de alojamiento":
        st.info("AquÃ­ irÃ­a la tabla / buscador de reseÃ±as.")
    elif accion == "AÃ±adir reseÃ±as a alojamiento":
        st.info("Formulario para aÃ±adir nuevas reseÃ±asâ€¦")
    elif accion == "Vincular reseÃ±a â†’ alojamiento":
        st.info("Herramienta para vincular reseÃ±a â†” alojamientoâ€¦")

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. SCRAPING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
elif seccion == "Scraping":
    st.title("ğŸ› ï¸ Scraping")

    st.write("HOLA")  # â† De momento solo esto

    # Ejemplo: futuro cuadro de bÃºsqueda
    # palabra = st.text_input("Palabra clave")
    # if st.button("Buscar"):
    #     urls = buscar_en_google(palabra)
    #     st.write(urls)
    #     guardar_urls_en_csv(urls)
